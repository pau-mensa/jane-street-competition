{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_schedule_sampling(iter: int, decoder_step: int, max_iters: int, max_decoder_step: int = 128) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the sampling probability for the golden token using the Composite schedule.\n",
    "    \n",
    "    Args:\n",
    "        iter (int): Current training iteration\n",
    "        decoder_step (int): Current decoding step\n",
    "        max_iters (int): Maximum number of training iterations\n",
    "        max_decoder_step (int): Maximum number of decoding steps\n",
    "    \n",
    "    Returns:\n",
    "        float: Probability of sampling the golden token\n",
    "    \"\"\"\n",
    "    # Normalize the training iteration to [0, 1]\n",
    "    normalized_iter = iter / max_iters #real_max_iters\n",
    "    \n",
    "    # Calculate f(i) - linear decay from 1 to 0 over training\n",
    "    f_i = max(0.0, 1.0 - normalized_iter)\n",
    "    \n",
    "    # Normalize decoder step to [0, 1]\n",
    "    normalized_step = decoder_step / max_decoder_step\n",
    "    \n",
    "    # Calculate h(i,t) = g(t * (1 - f(i)))\n",
    "    # Using sigmoid as g(x) to create smooth transition\n",
    "    x = normalized_step * (1 - f_i)\n",
    "    probability = 1 - (1 / (1 + np.exp(-10 * (x - 0.5))))\n",
    "    \n",
    "    # Clip probability to [0, 1] range\n",
    "    probability = np.clip(probability, 0.0, 1.0)\n",
    "    \n",
    "    return 1 - float(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "# Parts of the code are modifications of Pytorch's AdamW optimizer\n",
    "# Parts of the code are modifications of code from https://github.com/jiaweizzhao/GaLore/blob/master/galore_torch/galore_projector.py\n",
    "\n",
    "\n",
    "class SOAP(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements SOAP algorithm (https://arxiv.org/abs/2409.11321).\n",
    "\n",
    "    Parameters:\n",
    "        params (`Iterable[nn.parameter.Parameter]`):\n",
    "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "        lr (`float`, *optional*, defaults to 0.003):\n",
    "            The learning rate to use.\n",
    "        betas (`Tuple[float,float]`, *optional*, defaults to `(0.95, 0.95)`):\n",
    "            Adam's betas parameters (b1, b2).\n",
    "        shampoo_beta (`float`, *optional*, defaults to -1):\n",
    "            If >= 0, use this beta for the preconditioner (L and R in paper, state['GG'] below) moving average instead of betas[1].\n",
    "        eps (`float`, *optional*, defaults to 1e-08):\n",
    "            Adam's epsilon for numerical stability.\n",
    "        weight_decay (`float`, *optional*, defaults to 0.01): weight decay coefficient.\n",
    "        precondition_frequency (`int`, *optional*, defaults to 10):\n",
    "            How often to update the preconditioner.\n",
    "        max_precond_dim (`int`, *optional*, defaults to 10000):\n",
    "            Maximum dimension of the preconditioner.\n",
    "            Set to 10000, so that we exclude most common vocab sizes while including layers.\n",
    "        merge_dims (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to merge dimensions of the preconditioner.\n",
    "        precondition_1d (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to precondition 1D gradients.\n",
    "        normalize_grads (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to normalize gradients per layer. \n",
    "            Helps at large precondition_frequency (~100 in our experiments), \n",
    "            but hurts performance at small precondition_frequency (~10 in our experiments).\n",
    "        data_format (`str`, *optional*, defaults to `channels_first`):\n",
    "            Data format of the input for convolutional layers.\n",
    "            Should be \"channels_last\" for data_format of NHWC and \"channels_first\" for NCHW.\n",
    "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to use bias correction in Adam.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 3e-3,\n",
    "        betas=(0.95, 0.95),\n",
    "        shampoo_beta: float= -1,\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.01,\n",
    "        precondition_frequency: int=10,\n",
    "        max_precond_dim: int=10000, # \n",
    "        merge_dims: bool = False, # Merge dimensions till the product of the dimensions is less than or equal to max_precond_dim.\n",
    "        precondition_1d: bool = False,\n",
    "        normalize_grads: bool = False,\n",
    "        data_format: str = \"channels_first\",\n",
    "        correct_bias: bool = True,\n",
    "    ):\n",
    "        defaults = {\n",
    "            \"lr\": lr,\n",
    "            \"betas\": betas,\n",
    "            \"shampoo_beta\": shampoo_beta,\n",
    "            \"eps\": eps,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"precondition_frequency\": precondition_frequency,\n",
    "            \"max_precond_dim\": max_precond_dim,\n",
    "            \"merge_dims\": merge_dims,\n",
    "            \"precondition_1d\": precondition_1d,\n",
    "            \"normalize_grads\": normalize_grads,\n",
    "            \"correct_bias\": correct_bias,\n",
    "        }\n",
    "        super().__init__(params, defaults)\n",
    "        self._data_format = data_format\n",
    "        \n",
    "    def merge_dims(self, grad, max_precond_dim):\n",
    "        \"\"\"\n",
    "        Merges dimensions of the gradient tensor till the product of the dimensions is less than or equal to max_precond_dim.\n",
    "        \"\"\"\n",
    "        assert self._data_format in [\"channels_first\", \"channels_last\"]\n",
    "        if self._data_format == \"channels_last\" and grad.dim() == 4:\n",
    "            grad = grad.permute(0, 3, 1, 2)\n",
    "        shape = grad.shape\n",
    "        new_shape = []\n",
    "        \n",
    "        curr_shape = 1\n",
    "        for sh in shape:\n",
    "            temp_shape = curr_shape * sh\n",
    "            if temp_shape > max_precond_dim:\n",
    "                if curr_shape > 1:\n",
    "                    new_shape.append(curr_shape)\n",
    "                    curr_shape = sh\n",
    "                else:\n",
    "                    new_shape.append(sh)\n",
    "                    curr_shape = 1\n",
    "            else:\n",
    "                curr_shape = temp_shape\n",
    "        \n",
    "        if curr_shape > 1 or len(new_shape)==0:\n",
    "            new_shape.append(curr_shape)\n",
    "        \n",
    "        new_grad = grad.reshape(new_shape)\n",
    "        return new_grad               \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure = None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        if closure is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "\n",
    "                state = self.state[p]\n",
    "                \n",
    "                if \"step\" not in state:\n",
    "                    state[\"step\"] = 0 \n",
    "                    \n",
    "                # State initialization\n",
    "                if \"exp_avg\" not in state:\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(grad)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
    "                \n",
    "                if 'Q' not in state:\n",
    "                    self.init_preconditioner(\n",
    "                        grad,\n",
    "                        state,\n",
    "                        precondition_frequency=group['precondition_frequency'],\n",
    "                        precondition_1d=group['precondition_1d'],\n",
    "                        shampoo_beta=(group['shampoo_beta'] if group['shampoo_beta'] >= 0 else group[\"betas\"][1]),\n",
    "                        max_precond_dim=group['max_precond_dim'],\n",
    "                        merge_dims=group[\"merge_dims\"],\n",
    "                    )\n",
    "                    self.update_preconditioner(grad, state,\n",
    "                                               max_precond_dim=group['max_precond_dim'],\n",
    "                                               merge_dims=group[\"merge_dims\"],\n",
    "                                               precondition_1d=group[\"precondition_1d\"])\n",
    "                    continue # first step is skipped so that we never use the current gradients in the projection.\n",
    "                \n",
    "                # Projecting gradients to the eigenbases of Shampoo's preconditioner \n",
    "                # i.e. projecting to the eigenbases of matrices in state['GG']\n",
    "                grad_projected = self.project(grad, state, merge_dims=group[\"merge_dims\"], \n",
    "                                              max_precond_dim=group['max_precond_dim'])\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(grad_projected, alpha=(1.0 - beta1))\n",
    "                exp_avg_sq.mul_(beta2).add_(grad_projected.square(), alpha=(1.0 - beta2))\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                \n",
    "                # Projecting the exponential moving average of gradients to the eigenbases of Shampoo's preconditioner \n",
    "                # i.e. projecting to the eigenbases of matrices in state['GG']\n",
    "                # exp_avg_projected = self.project(exp_avg, state, merge_dims=group[\"merge_dims\"],\n",
    "                #                                  max_precond_dim=group['max_precond_dim'])\n",
    "                exp_avg_projected = exp_avg\n",
    "                \n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:\n",
    "                    bias_correction1 = 1.0 - beta1 ** (state[\"step\"])\n",
    "                    bias_correction2 = 1.0 - beta2 ** (state[\"step\"])\n",
    "                    step_size = step_size * (bias_correction2 ** .5) / bias_correction1\n",
    "\n",
    "                # Projecting back the preconditioned (by Adam) exponential moving average of gradients\n",
    "                # to the original space\n",
    "                norm_grad = self.project_back(exp_avg_projected / denom, state, merge_dims=group[\"merge_dims\"],\n",
    "                                                 max_precond_dim=group['max_precond_dim'])\n",
    "\n",
    "                if group[\"normalize_grads\"]:\n",
    "                    norm_grad = norm_grad / (1e-30+torch.mean(norm_grad**2)**0.5)\n",
    "                \n",
    "                p.add_(norm_grad, alpha=-step_size)\n",
    "                \n",
    "\n",
    "                # From AdamW code: Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if group[\"weight_decay\"] > 0.0:\n",
    "                    p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
    "                    \n",
    "                # Update is done after the gradient step to avoid using current gradients in the projection.\n",
    "                self.update_preconditioner(grad, state, \n",
    "                                               max_precond_dim=group['max_precond_dim'],\n",
    "                                               merge_dims=group[\"merge_dims\"],\n",
    "                                               precondition_1d=group[\"precondition_1d\"])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def init_preconditioner(self, grad, state, precondition_frequency=10, \n",
    "                            shampoo_beta=0.95, max_precond_dim=10000, precondition_1d=False,\n",
    "                            merge_dims=False):\n",
    "        \"\"\"\n",
    "        Initializes the preconditioner matrices (L and R in the paper).\n",
    "        \"\"\"\n",
    "        state['GG'] = [] # Will hold all the preconditioner matrices (L and R in the paper).\n",
    "        if grad.dim() == 1:\n",
    "            if not precondition_1d or grad.shape[0] > max_precond_dim:\n",
    "                state['GG'].append([])\n",
    "            else:\n",
    "                state['GG'].append(torch.zeros(grad.shape[0], grad.shape[0], device=grad.device))\n",
    "        else:\n",
    "            if merge_dims:\n",
    "                grad = self.merge_dims(grad, max_precond_dim)\n",
    "\n",
    "            for sh in grad.shape:\n",
    "                if sh > max_precond_dim:\n",
    "                    state['GG'].append([])\n",
    "                else:\n",
    "                    state['GG'].append(torch.zeros(sh, sh, device=grad.device))\n",
    "                    \n",
    "        state['Q'] = None # Will hold all the eigenbases of the preconditioner.\n",
    "        state['precondition_frequency'] = precondition_frequency\n",
    "        state['shampoo_beta'] = shampoo_beta          \n",
    "        \n",
    "    def project(self, grad, state, merge_dims=False, max_precond_dim=10000):\n",
    "        \"\"\"\n",
    "        Projects the gradient to the eigenbases of the preconditioner.\n",
    "        \"\"\"\n",
    "        original_shape = grad.shape\n",
    "        if merge_dims:\n",
    "            if grad.dim() == 4 and self._data_format == 'channels_last':\n",
    "                permuted_shape = grad.permute(0, 3, 1, 2).shape\n",
    "            grad = self.merge_dims(grad, max_precond_dim)\n",
    "\n",
    "        for mat in state['Q']:\n",
    "            if len(mat) > 0:\n",
    "                grad = torch.tensordot(\n",
    "                        grad,\n",
    "                        mat,\n",
    "                        dims=[[0], [0]],\n",
    "                    )\n",
    "            else:\n",
    "                permute_order = list(range(1, len(grad.shape))) + [0]\n",
    "                grad = grad.permute(permute_order)\n",
    "        \n",
    "        if merge_dims:\n",
    "            if self._data_format == 'channels_last' and len(original_shape) == 4:\n",
    "                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)\n",
    "            else:\n",
    "                grad = grad.reshape(original_shape)\n",
    "        return grad\n",
    "        \n",
    "    def update_preconditioner(self, grad, state, \n",
    "                              max_precond_dim=10000, merge_dims=False, precondition_1d=False):\n",
    "        \"\"\"\n",
    "        Updates the preconditioner matrices and the eigenbases (L, R, Q_L, Q_R in the paper).\n",
    "        \"\"\"\n",
    "        if state[\"Q\"] is not None:\n",
    "            state[\"exp_avg\"] = self.project_back(state[\"exp_avg\"], state, merge_dims=merge_dims, max_precond_dim=max_precond_dim)\n",
    "        if grad.dim() == 1:\n",
    "            if precondition_1d and grad.shape[0] <= max_precond_dim:\n",
    "                state['GG'][0].lerp_(grad.unsqueeze(1) @ grad.unsqueeze(0), 1-state['shampoo_beta'])\n",
    "        else:\n",
    "            if merge_dims:\n",
    "                new_grad = self.merge_dims(grad, max_precond_dim)\n",
    "                for idx, sh in enumerate(new_grad.shape):\n",
    "                    if sh <= max_precond_dim:\n",
    "                        outer_product = torch.tensordot(\n",
    "                                new_grad,\n",
    "                                new_grad,\n",
    "                                dims=[[*chain(range(idx), range(idx + 1, len(new_grad.shape)))]] * 2,\n",
    "                            )\n",
    "                        state['GG'][idx].lerp_(outer_product, 1-state['shampoo_beta'])\n",
    "            else:\n",
    "                for idx, sh in enumerate(grad.shape):\n",
    "                    if sh <= max_precond_dim:\n",
    "                        outer_product = torch.tensordot(\n",
    "                                grad,\n",
    "                                grad,\n",
    "                                # Contracts across all dimensions except for k.\n",
    "                                dims=[[*chain(range(idx), range(idx + 1, len(grad.shape)))]] * 2,\n",
    "                            )\n",
    "                        state['GG'][idx].lerp_(outer_product, 1-state['shampoo_beta'])\n",
    "                     \n",
    "        if state['Q'] is None:\n",
    "            state['Q'] = self.get_orthogonal_matrix(state['GG'])\n",
    "        if state['step'] > 0 and state['step'] % state['precondition_frequency'] == 0:\n",
    "            state['Q'] = self.get_orthogonal_matrix_QR(state, max_precond_dim, merge_dims)\n",
    "            # state['Q'] = self.get_fast_QR(state, max_precond_dim, merge_dims)             \n",
    "\n",
    "        if state[\"step\"] > 0:\n",
    "            state[\"exp_avg\"] = self.project(state[\"exp_avg\"], state, merge_dims=merge_dims, max_precond_dim=max_precond_dim) \n",
    "\n",
    "    def project_back(self, grad, state, merge_dims=False, max_precond_dim=10000):\n",
    "        \"\"\"\n",
    "        Projects the gradient back to the original space.\n",
    "        \"\"\"\n",
    "        original_shape = grad.shape\n",
    "        if merge_dims:\n",
    "            if self._data_format == 'channels_last' and grad.dim() == 4:\n",
    "                permuted_shape = grad.permute(0, 3, 1, 2).shape\n",
    "            grad = self.merge_dims(grad, max_precond_dim)\n",
    "        for mat in state['Q']:\n",
    "            if len(mat) > 0:\n",
    "                grad = torch.tensordot(\n",
    "                        grad,\n",
    "                        mat,\n",
    "                        dims=[[0], [1]],\n",
    "                    )\n",
    "            else:\n",
    "                permute_order = list(range(1, len(grad.shape))) + [0]\n",
    "                grad = grad.permute(permute_order)\n",
    "                \n",
    "        if merge_dims:\n",
    "            if self._data_format == 'channels_last' and len(original_shape) == 4:\n",
    "                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)\n",
    "            else:\n",
    "                grad = grad.reshape(original_shape)\n",
    "        return grad\n",
    "        \n",
    "\n",
    "    def get_orthogonal_matrix(self, mat):\n",
    "        \"\"\"\n",
    "        Computes the eigenbases of the preconditioner using torch.linalg.eigh decomposition.\n",
    "        \"\"\"\n",
    "        matrix = []\n",
    "        for m in mat:\n",
    "            if len(m) == 0:\n",
    "                matrix.append([])\n",
    "                continue\n",
    "            if m.data.dtype != torch.float:\n",
    "                float_data = False\n",
    "                original_type = m.data.dtype\n",
    "                original_device = m.data.device\n",
    "                matrix.append(m.data.float())\n",
    "            else:\n",
    "                float_data = True\n",
    "                matrix.append(m.data)\n",
    "        \n",
    "        final = []\n",
    "        for m in matrix:\n",
    "            if len(m) == 0:\n",
    "                final.append([])\n",
    "                continue\n",
    "            try:\n",
    "                _, Q = torch.linalg.eigh(m+1e-30*torch.eye(m.shape[0], device=m.device))\n",
    "            except:\n",
    "                _, Q = torch.linalg.eigh(m.to(torch.float64)+1e-30*torch.eye(m.shape[0], device=m.device))\n",
    "                Q = Q.to(m.dtype)\n",
    "            Q = torch.flip(Q, [1])\n",
    "\n",
    "            if not float_data:\n",
    "                Q = Q.to(original_device).type(original_type)\n",
    "            final.append(Q)\n",
    "        return final\n",
    "        \n",
    "\n",
    "    def get_orthogonal_matrix_QR(self, state, max_precond_dim=10000, merge_dims=False):\n",
    "        \"\"\"\n",
    "        Computes the eigenbases of the preconditioner using one round of power iteration \n",
    "        followed by torch.linalg.qr decomposition.\n",
    "        \"\"\"\n",
    "        precond_list = state['GG']\n",
    "        orth_list = state['Q']\n",
    "\n",
    "        matrix = []\n",
    "        orth_matrix = []\n",
    "        for m,o in zip(precond_list, orth_list):\n",
    "            if len(m) == 0:\n",
    "                matrix.append([])\n",
    "                orth_matrix.append([])\n",
    "                continue\n",
    "            if m.data.dtype != torch.float:\n",
    "                float_data = False\n",
    "                original_type = m.data.dtype\n",
    "                original_device = m.data.device\n",
    "                matrix.append(m.data.float())\n",
    "                orth_matrix.append(o.data.float())\n",
    "            else:\n",
    "                float_data = True\n",
    "                matrix.append(m.data.float())\n",
    "                orth_matrix.append(o.data.float())\n",
    "        \n",
    "        orig_shape = state['exp_avg_sq'].shape\n",
    "        if self._data_format == 'channels_last' and len(orig_shape) == 4:\n",
    "            permuted_shape = state['exp_avg_sq'].permute(0, 3, 1, 2).shape\n",
    "        if merge_dims:\n",
    "            exp_avg_sq = self.merge_dims(state['exp_avg_sq'], max_precond_dim)\n",
    "        else:\n",
    "            exp_avg_sq = state['exp_avg_sq']\n",
    "            \n",
    "        final = []\n",
    "        for ind, (m,o) in enumerate(zip(matrix, orth_matrix)):\n",
    "            if len(m)==0:\n",
    "                final.append([])\n",
    "                continue\n",
    "            est_eig = torch.diag(o.T @ m @ o)\n",
    "            sort_idx = torch.argsort(est_eig, descending=True)\n",
    "            exp_avg_sq = exp_avg_sq.index_select(ind, sort_idx)\n",
    "            o = o[:,sort_idx]\n",
    "            power_iter = m @ o\n",
    "            Q, _ = torch.linalg.qr(power_iter)\n",
    "\n",
    "            if not float_data:\n",
    "                Q = Q.to(original_device).type(original_type)\n",
    "            final.append(Q)\n",
    "        \n",
    "        if merge_dims:\n",
    "            if self._data_format == 'channels_last' and len(orig_shape) == 4:\n",
    "                exp_avg_sq = exp_avg_sq.reshape(permuted_shape).permute(0, 2, 3, 1)\n",
    "            else:\n",
    "                exp_avg_sq = exp_avg_sq.reshape(orig_shape)\n",
    "                \n",
    "        state['exp_avg_sq'] = exp_avg_sq\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def downsample_array(arr: np.ndarray, period: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Downsample a numpy array by taking the mean over specified periods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : np.ndarray\n",
    "        Input array to be downsampled\n",
    "    period : int\n",
    "        Number of elements to average over\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Downsampled array where each element is the mean of 'period' elements\n",
    "        from the original array\n",
    "        \n",
    "    Examples:\n",
    "    --------\n",
    "    >>> arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "    >>> downsample_array(arr, 2)\n",
    "    array([1.5, 3.5, 5.5])\n",
    "    >>> downsample_array(arr, 3)\n",
    "    array([2., 5.])\n",
    "    \"\"\"\n",
    "    # Check if array length is divisible by period\n",
    "    if len(arr) % period != 0:\n",
    "        # Trim array to make it divisible by period\n",
    "        trim_length = len(arr) - (len(arr) % period)\n",
    "        arr = arr[:trim_length]\n",
    "    \n",
    "    # Reshape array into rows of length 'period'\n",
    "    reshaped = arr.reshape(-1, period)\n",
    "    \n",
    "    # Calculate mean along each row\n",
    "    return np.mean(reshaped, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6y5GjM7yX2Yd",
    "outputId": "c8d7286e-7a4d-4f94-ee17-1e9aa8bf2c8b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "DATA = pl.scan_parquet('train.parquet/')\n",
    "\n",
    "SYMBOLS = sorted(DATA.select('symbol_id').unique().collect().to_numpy().flatten())\n",
    "print(SYMBOLS)\n",
    "FEATURE_COLS = [col for col in DATA.collect_schema().names() if 'feature_' in col] + [col + '_downsampled' for col in DATA.collect_schema().names() if 'responder' in col]# + ['weight']\n",
    "\n",
    "RESPONDER_COLS = [col for col in DATA.collect_schema().names() if 'responder' in col]\n",
    "DFS_USED = {}\n",
    "DATES_USED = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(SYMBOLS, total=len(SYMBOLS), desc='Indexing DFS...'):\n",
    "    # We load each dataframe into memory, probably not the best approach... (We can get away with it since the amount of data is small)\n",
    "    lazy = pl.scan_parquet(f'df_train.parquet/{k}.parquet')\n",
    "    df = lazy.sort(['date_id', 'time_id']).collect()\n",
    "    DFS_USED[k] = df\n",
    "    DATES_USED[k] = sorted(lazy.select('date_id').unique().collect().to_numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_IDXS = {}\n",
    "for k in tqdm(SYMBOLS, total=len(SYMBOLS)):\n",
    "    min_idx = DFS_USED[k].filter(pl.col('date_id') == DATES_USED[k][1]).select(pl.col('index').max()).to_numpy().flatten()[0]\n",
    "    MIN_IDXS[k] = min_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LEN = 0\n",
    "for df in DFS_USED.values():\n",
    "    DATA_LEN += df.shape[0]\n",
    "DATA_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYQKUnhfiJeL"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import re\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoWnBdCuboNj"
   },
   "outputs": [],
   "source": [
    "class Rotary(nn.Module):\n",
    "    \"\"\"Module that implements RoPE (Rotary Positional Embeddings)\"\"\"\n",
    "    def __init__(self, dim, base=10_000):\n",
    "        super().__init__()\n",
    "        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.outer(t, self.inv_freq).to(x.device)\n",
    "            self.cos_cached = freqs.cos()\n",
    "            self.sin_cached = freqs.sin()\n",
    "        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n",
    "\n",
    "def apply_rotary_emb(x, cos, sin):\n",
    "    assert x.ndim == 4\n",
    "    d = x.shape[3] // 2\n",
    "    x1 = x[..., :d]\n",
    "    x2 = x[..., d:]\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return torch.cat([y1, y2], 3).type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLCr-S-ccAG6"
   },
   "outputs": [],
   "source": [
    "def zero_weighted_rsquared(y_pred, y_true, weights):\n",
    "    # Calculate numerator: Σ w_i(y_i - ŷ_i)^2\n",
    "    numerator = torch.sum(weights * (y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate denominator: Σ w_i * y_i^2\n",
    "    denominator = torch.sum(weights * y_true**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (numerator / denominator)\n",
    "\n",
    "    # Since we want to minimize the loss, we return 1 - R^2\n",
    "    return 1 - r_squared\n",
    "\n",
    "def zero_weighted_rsquared_np(y_pred, y_true, weights):\n",
    "    # Calculate numerator: Σ w_i(y_i - ŷ_i)^2\n",
    "    numerator = np.sum(weights * (y_true - y_pred)**2)\n",
    "\n",
    "    # Calculate denominator: Σ w_i * y_i^2\n",
    "    denominator = np.sum(weights * y_true**2)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (numerator / denominator)\n",
    "\n",
    "    # Since we want to minimize the loss, we return 1 - R^2\n",
    "    return 1 - r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH1ggo6UzO6x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable\n",
    "\n",
    "class GatedNormalization(nn.Module):\n",
    "    \"\"\"Gated Normalization module that balances between BatchNorm and InstanceNorm using a gate in order to handle severe non-stationary features.\"\"\"\n",
    "    def __init__(self, num_features, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.feature_wise_norm = nn.InstanceNorm1d(num_features)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features)\n",
    "        self.gate = nn.Parameter(torch.zeros(num_features))  # Initialized to 0 (equal weighting)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, features, time)\n",
    "        rms_normed = self.feature_wise_norm(x)\n",
    "        layer_normed = self.batch_norm(x)\n",
    "\n",
    "        gate = torch.sigmoid(self.gate).view(1, -1, 1)  # (1, features, 1)\n",
    "        return gate * rms_normed + (1 - gate) * layer_normed\n",
    "\n",
    "class BlockTimeReducer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that reduces the time dimension of a tensor of size (batch, features, time, embd).\n",
    "    The idea here is to leverage the high auto correlation of the features across time in order to reduce computation later in the attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction_steps, n_features=79, n_embd=64, activation='gelu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Learnable feature-wise scaling factors\n",
    "        self.feature_scales = nn.Parameter(torch.ones(n_features, 1, 1))\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'gelu_new':\n",
    "            self.activation = GELUNew()\n",
    "        elif activation == 'silu':\n",
    "            self.activation = SiLU()\n",
    "        elif activation == 'mish':\n",
    "            self.activation = nn.Mish()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels=n_features,\n",
    "                out_channels=n_features,\n",
    "                kernel_size=(step, 1),\n",
    "                stride=(step, 1),\n",
    "                groups=n_features,\n",
    "            ) for step in reduction_steps\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, n_features, block_size, n_embd)\n",
    "        batch_size, n_features, time, n_embd = x.shape\n",
    "        x = x * self.feature_scales\n",
    "        \n",
    "        # Apply reduction strategy\n",
    "        for c in self.conv_layers:\n",
    "            x = c(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, norm_dim=0):\n",
    "        super().__init__()\n",
    "        self.norm_dim = norm_dim\n",
    "        self.scale = dim**0.5\n",
    "        if norm_dim == 0:\n",
    "            self.g = nn.Parameter(torch.ones(dim))\n",
    "        else:\n",
    "            self.g = nn.Parameter(torch.ones(dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.norm_dim == 0:\n",
    "            return F.normalize(x, dim=-1) * self.scale * self.g\n",
    "        else:\n",
    "            return F.normalize(x, dim=1) * self.scale * self.g\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation: Callable, mult_bias=False):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "        self.mult_bias = nn.Parameter(torch.ones(dim_out)) if mult_bias else 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * self.act(gate) * self.mult_bias\n",
    "    \n",
    "class InformedDownsampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that reduces along the feature dimension of a tensor of size (batch, feature, time, embd).\n",
    "    The features.csv and responders.csv suggest that some features belong in the same categories, so it makes sense to reduce them to fit those categories.\n",
    "    The idea is the same as in the Time reducer, reduce the size of the tensors to reduce the attention computation.\n",
    "    Note: The first layer weights' are initialized using the correlation matrix taken from the .csv files.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim=8, feature_relationships=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Process feature relationships if provided\n",
    "        self.correlation_matrix = None\n",
    "        if feature_relationships is not None:\n",
    "            # Convert boolean strings to actual booleans and then to float tensor\n",
    "            tag_matrix = feature_relationships.iloc[:, 1:].astype(bool).astype(float).values\n",
    "            # Calculate feature correlations based on tag similarities\n",
    "            self.correlation_matrix = torch.from_numpy(\n",
    "                np.corrcoef(tag_matrix)\n",
    "            ).float()\n",
    "        \n",
    "        hidden_dim = input_dim // 2\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim, elementwise_affine=True),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2, elementwise_affine=True),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        \n",
    "        # Initialize weights using feature relationships if available\n",
    "        if self.correlation_matrix is not None:\n",
    "            self._initialize_weights()\n",
    "            \n",
    "        # Skip connection\n",
    "        self.skip = nn.Linear(input_dim, output_dim)\n",
    "            \n",
    "    def _initialize_weights(self):\n",
    "        # Fix initialization to match dimensions correctly\n",
    "        with torch.no_grad():\n",
    "            # Get the first layer's weight shape\n",
    "            out_features, in_features = self.layer1[0].weight.shape\n",
    "            \n",
    "            # Ensure correlation matrix matches input dimension\n",
    "            if self.correlation_matrix.shape[0] != in_features:\n",
    "                # If dimensions don't match, we need to either truncate or pad\n",
    "                if self.correlation_matrix.shape[0] < in_features:\n",
    "                    # Pad with zeros if correlation matrix is too small\n",
    "                    padded = torch.zeros(in_features, in_features)\n",
    "                    padded[:self.correlation_matrix.shape[0], :self.correlation_matrix.shape[1]] = self.correlation_matrix\n",
    "                    self.correlation_matrix = padded\n",
    "                else:\n",
    "                    # Truncate if correlation matrix is too large\n",
    "                    self.correlation_matrix = self.correlation_matrix[:in_features, :in_features]\n",
    "            \n",
    "            # Scale and reshape correlation matrix to match first layer's weights\n",
    "            scaled_weights = (self.correlation_matrix * 0.1).T\n",
    "            self.layer1[0].weight.data = scaled_weights[:out_features, :]\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Handle 4D input (batch, n_features, time, embd)\n",
    "        batch_size, n_features, time_steps, embedding_dim = x.shape\n",
    "        \n",
    "        # Reshape to process features while maintaining causality\n",
    "        # First, move feature dim to end: (batch, time, embd, n_features)\n",
    "        x_reshaped = x.permute(0, 2, 3, 1)\n",
    "\n",
    "        # Process through layers\n",
    "        h = self.layer1(x_reshaped)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        skip = self.skip(x_reshaped)\n",
    "        h = h + skip\n",
    "\n",
    "        # Permute back to original dimension order: (batch, output_dim, time, embd)\n",
    "        return h.permute(0, 3, 1, 2)\n",
    "\n",
    "def create_downsampler(csv_path, input_dim, output_dim=1):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if 'feature' in csv_path:\n",
    "        # Extract the 10th, 11th, and 12th rows\n",
    "        rows_to_move = df.iloc[9:12].copy()\n",
    "\n",
    "        # Drop these rows from the original DataFrame\n",
    "        df = df.drop(df.index[9:12])\n",
    "\n",
    "        # Append the extracted rows to the bottom of the DataFrame\n",
    "        df = pd.concat([df, rows_to_move], ignore_index=True)\n",
    "        \n",
    "    return InformedDownsampler(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        feature_relationships=df\n",
    "    )\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config, n_embd=None):\n",
    "        super().__init__()\n",
    "        if n_embd is None:\n",
    "            n_embd = config.n_embd\n",
    "        self.glu     = GLU(n_embd, 4 * n_embd, nn.SiLU())\n",
    "        self.norm    = LayerNorm(4 * n_embd, bias=config.bias)\n",
    "        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.glu(x)\n",
    "        x = self.c_proj(self.norm(x))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config, is_causal):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.to_q = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.to_k = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.to_v = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.rotary = Rotary(config.n_embd // config.n_head)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        self.lamb1 = nn.Parameter(torch.tensor(0.5))\n",
    "        self.lamb2 = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "        # regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.dropout = config.dropout\n",
    "        self.is_causal = is_causal\n",
    "        # flash attention is way faster than MultiHeadAttention, requires Pytorch >= 2.0\n",
    "        assert hasattr(torch.nn.functional, 'scaled_dot_product_attention'), \"Flash Attention requires PyTorch >= 2.0\"\n",
    "\n",
    "\n",
    "    def forward(self, x, v1=None, cutoff=None):\n",
    "        if len(x.shape) == 3:\n",
    "            B, T, C = x.size()\n",
    "            FS = 1\n",
    "            reshape_at_end = False\n",
    "        else:\n",
    "            B, FS, T, C = x.size()  # batch, features, time, embedding\n",
    "        \n",
    "            # Reshape to combine features and time dimensions\n",
    "            x = x.permute(0, 2, 1, 3).reshape(B, T * FS, C)\n",
    "            reshape_at_end = True\n",
    "        \n",
    "        # Apply linear projections to reshaped input\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # Reshape to (batch, time*features, n_head, head_dim)\n",
    "        k = k.view(B, T * FS, self.n_head, C // self.n_head)\n",
    "        q = q.view(B, T * FS, self.n_head, C // self.n_head)\n",
    "        v = v.view(B, T * FS, self.n_head, C // self.n_head)\n",
    "        \n",
    "        if v1 is None:\n",
    "            v1 = v\n",
    "        else:\n",
    "            v1 = v1.view(B, T * FS, self.n_head, C // self.n_head)\n",
    "            \n",
    "        v = self.lamb1 * v + self.lamb2 * v1.view_as(v)\n",
    "\n",
    "        # Convert to float32 for rotary embeddings\n",
    "        orig_dtype = q.dtype\n",
    "        q_f32 = q.float()\n",
    "        k_f32 = k.float()\n",
    "        \n",
    "        # Get and apply rotary embeddings in float32\n",
    "        cos, sin = self.rotary(q_f32)\n",
    "        q = apply_rotary_emb(q_f32, cos, sin).to(orig_dtype)\n",
    "        k = apply_rotary_emb(k_f32, cos, sin).to(orig_dtype)\n",
    "        \n",
    "        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),))\n",
    "\n",
    "        # Transpose for attention\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        mask = None\n",
    "        \n",
    "        # Compute attention\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=mask,\n",
    "            dropout_p=self.dropout if self.training else 0,\n",
    "            is_causal=False\n",
    "        )\n",
    "        \n",
    "        # Reshape back\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T * FS, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        if reshape_at_end:\n",
    "            # Reshape back to original dimensions (batch, features, time, embedding)\n",
    "            y = y.view(B, T, FS, C).permute(0, 2, 1, 3)\n",
    "\n",
    "        return y, v1\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.to_q = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.to_k = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.to_v = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.rotary_q = Rotary(config.n_embd // config.n_head)\n",
    "        self.rotary_k = Rotary(config.n_embd // config.n_head)\n",
    "        \n",
    "        # regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        assert hasattr(torch.nn.functional, 'scaled_dot_product_attention'), \"Flash Attention requires PyTorch >= 2.0\"\n",
    "\n",
    "    def forward(self, x, encoded_x):\n",
    "        B, FS, eT, eC = encoded_x.size()  # batch, features, time, embedding for encoded_x\n",
    "        B, T, C = x.size()  # batch, time, embedding for x\n",
    "        \n",
    "        # Reshape encoded_x to combine features and time\n",
    "        encoded_x_reshaped = encoded_x.permute(0, 2, 1, 3).reshape(B, eT * FS, eC)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        q = self.to_q(x)  # (B, T, C)\n",
    "        k = self.to_k(encoded_x_reshaped)  # (B, eT*F, eC)\n",
    "        v = self.to_v(encoded_x_reshaped)  # (B, eT*F, eC)\n",
    "\n",
    "        # Reshape to add head dimension\n",
    "        k = k.view(B, eT * FS, self.n_head, eC // self.n_head)  # (B, eT*F, nh, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head)        # (B, T, nh, hs)\n",
    "        v = v.view(B, eT * FS, self.n_head, eC // self.n_head)  # (B, eT*F, nh, hs)\n",
    "        \n",
    "        # Apply separate rotary embeddings\n",
    "        q_f32 = q.float()\n",
    "        k_f32 = k.float()\n",
    "        \n",
    "        # Get rotary embeddings for different sequence lengths\n",
    "        cos_q, sin_q = self.rotary_q(q_f32)\n",
    "        cos_k, sin_k = self.rotary_k(k_f32)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        q = apply_rotary_emb(q_f32, cos_q, sin_q).to(q.dtype)\n",
    "        k = apply_rotary_emb(k_f32, cos_k, sin_k).to(k.dtype)\n",
    "        \n",
    "        # Apply RMS normalization\n",
    "        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),))\n",
    "\n",
    "        # Transpose for attention\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        k = k.transpose(1, 2)  # (B, nh, eT*F, hs)\n",
    "        v = v.transpose(1, 2)  # (B, nh, eT*F, hs)\n",
    "\n",
    "        # Compute attention\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.dropout if self.training else 0,\n",
    "            is_causal=False\n",
    "        )\n",
    "\n",
    "        # Reshape back\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection and dropout\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config, n_encoded_arrays=1):\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embd)\n",
    "        self.attn = SelfAttention(config, False)\n",
    "        self.rms_2 = RMSNorm(config.n_embd)\n",
    "        self.cross_attn = nn.ModuleList([CrossAttention(config) for _ in range(n_encoded_arrays)])\n",
    "        self.rms_3 = RMSNorm(config.n_embd)\n",
    "        self.rms_4 = RMSNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        if n_encoded_arrays > 1:\n",
    "            self.reducer = nn.Linear(config.downsampled_block_size * 2, config.downsampled_block_size)\n",
    "\n",
    "    def forward(self, x, encoded_x, self_v1=None, cutoff=None):\n",
    "        # We add x to each layer to skip connections.\n",
    "        if isinstance(encoded_x, list):\n",
    "            attn_out, sv1 = self.attn(self.rms_1(x), self_v1, cutoff)\n",
    "            x = x + attn_out\n",
    "            results = []\n",
    "            for idx, encoded_arr in enumerate(encoded_x):\n",
    "                cross_attn_out = self.cross_attn[idx](self.rms_2(x), self.rms_3(encoded_arr))#, cross_v1)\n",
    "                temp_x = x + cross_attn_out\n",
    "                temp_x = temp_x + self.mlp(self.rms_4(temp_x))\n",
    "                results.append(temp_x)\n",
    "            x = torch.cat(results, dim=1)\n",
    "            x = self.reducer(x.transpose(1, 2)).transpose(1, 2)\n",
    "        else:\n",
    "            attn_out, sv1 = self.attn(self.rms_1(x), self_v1)\n",
    "            x = x + attn_out\n",
    "            cross_attn_out = self.cross_attn[0](self.rms_2(x), self.rms_3(encoded_x))#, cross_v1)\n",
    "            x = x + cross_attn_out\n",
    "            x = x + self.mlp(self.rms_4(x))\n",
    "        return x, sv1\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config, causal_attn=False):\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embd)\n",
    "        self.attn = SelfAttention(config, causal_attn)\n",
    "        self.rms_2 = RMSNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, v1=None):\n",
    "        attn_out, v1 = self.attn(self.rms_1(x), v1)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x, v1\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_cat_features = config.n_cat_features\n",
    "        self.n_cont_features = config.n_cont_features\n",
    "        self.vocab_sizes = config.vocab_sizes\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "        self.downsampled_block_size = config.downsampled_block_size\n",
    "        self.dropout = config.dropout\n",
    "        self.n_responder_features = config.n_responder_features\n",
    "        self.n_layer = config.n_layer\n",
    "        assert self.n_cat_features == len(self.vocab_sizes)\n",
    "        assert config.n_layer == config.n_layer\n",
    "        assert config.block_size % config.downsampled_block_size == 0\n",
    "        self.max_iters = config.n_iters\n",
    "\n",
    "        self.cont_norm = RMSNorm(config.n_embd)\n",
    "        self.cat_norm = RMSNorm(config.n_embd)\n",
    "        self.full_norm = RMSNorm(config.n_embd)\n",
    "        \n",
    "        # Categorical features embedding\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, config.n_embd)\n",
    "            for vocab_size in config.vocab_sizes\n",
    "        ])\n",
    "        self.nan_embedding = nn.Embedding(2, config.n_embd)\n",
    "        self.synthetic_embedding = nn.Embedding(2, config.n_embd)\n",
    "        \n",
    "        self.encoder = nn.ModuleDict(dict(\n",
    "            fc_cont = nn.Linear(1, config.n_embd),\n",
    "            wpe = nn.Embedding(968, config.n_embd),\n",
    "            down_time_cont = BlockTimeReducer(\n",
    "                [2], \n",
    "                config.n_cont_features + config.n_cat_features,\n",
    "                n_embd=config.n_embd,\n",
    "                activation='gelu'\n",
    "            ),\n",
    "            down_time_resp = BlockTimeReducer(\n",
    "                [2], #[4, 4],\n",
    "                n_features=config.n_responder_features,\n",
    "                n_embd=config.n_embd,\n",
    "                activation='gelu'\n",
    "            ),\n",
    "            down_f_cont = create_downsampler('features.csv', config.n_cont_features + config.n_cat_features, 16),\n",
    "            down_f_resp = create_downsampler('responders.csv', config.n_responder_features, 4),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([EncoderBlock(config, causal_attn=False) for _ in range(config.n_layer)]),\n",
    "        ))\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            wte = nn.Linear(1, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = RMSNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        self.regressor = nn.Linear(config.vocab_size, 1)\n",
    "\n",
    "    def forward(self, s, cat_features, cont_features, nan_cont_features, responders, t_ids, it=None, synthetic_flag=None):\n",
    "        assert cat_features.shape[1] == self.n_cat_features\n",
    "        batch_size, _, block_size = cont_features.shape\n",
    "        \n",
    "        if synthetic_flag is None and it is not None:\n",
    "            # Not used in scheduled sampling.\n",
    "            seq, synthetic_flag, cutoff = self.generate_synthetic_data(s, it)\n",
    "        else:\n",
    "            seq = s.clone()\n",
    "            cutoff = None\n",
    "            if synthetic_flag is None:\n",
    "                synthetic_flag = torch.zeros((batch_size, self.downsampled_block_size), dtype=torch.int64, device=cont_features.device)\n",
    "                \n",
    "        synthetic_flag = self.synthetic_embedding(synthetic_flag)\n",
    "        \n",
    "        cont_features = self.encoder.fc_cont(cont_features.unsqueeze(-1))\n",
    "        responders = self.decoder.wte(responders.unsqueeze(-1))\n",
    "        \n",
    "        nan_cont_features_embd = self.nan_embedding(nan_cont_features)\n",
    "        pos_features = self.encoder.wpe(t_ids)\n",
    "        \n",
    "        # Process categorical features\n",
    "        batch_size, num_features, _ = cat_features.shape\n",
    "        embedded_features = [\n",
    "            self.cat_embeddings[i](cat_features[:, i])  # (batch_size, block_size, n_embd)\n",
    "            for i in range(num_features)\n",
    "        ]\n",
    "        \n",
    "        # Concatenate embeddings along the last dimension\n",
    "        cat_features = torch.stack(embedded_features, dim=1)\n",
    "        cat_features = self.cat_norm(cat_features + pos_features.unsqueeze(1))\n",
    "        \n",
    "        cont_features = self.cont_norm(cont_features + nan_cont_features_embd + pos_features.unsqueeze(1))\n",
    "        cont_features = cont_features * nan_cont_features.unsqueeze(-1)\n",
    "        cont_features = self.full_norm(torch.cat([cont_features, cat_features], dim=1))\n",
    "        \n",
    "        cont_features = self.encoder.down_time_cont(cont_features)\n",
    "        cont_features = self.encoder.down_f_cont(cont_features).squeeze(1)\n",
    "        \n",
    "        responders = self.encoder.down_time_resp(responders)\n",
    "        responders = self.encoder.down_f_resp(responders).squeeze(1)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        pos = torch.arange(0, self.downsampled_block_size, dtype=torch.long, device=cont_features.device)\n",
    "        pos_emb = self.decoder.wpe(pos)\n",
    "\n",
    "        encoded_data = self.encoder.drop(torch.cat([cont_features, responders], dim=1))\n",
    "        target = None\n",
    "        \n",
    "        tok_emb = self.decoder.wte(seq.unsqueeze(-1))\n",
    "        x = self.decoder.drop(tok_emb + pos_emb + synthetic_flag)\n",
    "\n",
    "        ve1 = None\n",
    "        vd1 = None\n",
    "        ve2 = None\n",
    "        for idx in range(self.n_layer):   \n",
    "            encoded_data, ve1 = self.encoder.h[idx](encoded_data, ve1)\n",
    "            x, vd1 = self.decoder.h[idx](x, encoded_data, vd1, cutoff)\n",
    "\n",
    "        x = self.decoder.ln_f(x)\n",
    "            \n",
    "        logits = self.lm_head(x)\n",
    "        target = torch.clamp(self.regressor(logits), -5, 5)\n",
    "\n",
    "        return target[:, -1:]\n",
    "\n",
    "    \n",
    "    def generate_synthetic_data(self, seq_tensor, it):\n",
    "        \"\"\"\n",
    "        Cuts a sequence on a generated idx in order to simulate inference. Not used anymore.\n",
    "        \"\"\"\n",
    "        batch, block_size = seq_tensor.size()\n",
    "\n",
    "        # Calculate cutting probability using cosine schedule\n",
    "        # At it=0: prob=0 (no cutting)\n",
    "        # At it=max_iters: prob=1 (all sequences cut)\n",
    "        progress = it / self.max_iters\n",
    "        cut_prob = 0.5 * (1 + torch.cos(torch.tensor(math.pi * (1 - progress))))\n",
    "\n",
    "        # Generate random values to determine which sequences to cut\n",
    "        should_cut = torch.rand(batch) < cut_prob\n",
    "\n",
    "        # Generate random indices in the range [0, block_size - 1] for sequences that will be cut\n",
    "        random_indices = torch.randint(0, block_size, (batch,))\n",
    "\n",
    "        # Create a new tensor with the same values as the original\n",
    "        repeated_tokens = seq_tensor.clone()\n",
    "\n",
    "        # Generate synthetic_flag tensor (0 for original sequence, 1 for extended sequence)\n",
    "        synthetic_flag = torch.zeros_like(seq_tensor)\n",
    "        \n",
    "        cutoff = None\n",
    "\n",
    "        return repeated_tokens, synthetic_flag.to(dtype=torch.int64), torch.tensor(cutoff, dtype=torch.int64, device=seq_tensor.device, requires_grad=False)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        \n",
    "        attention_params = []\n",
    "        decay_params = []\n",
    "        nodecay_params = []\n",
    "\n",
    "        # Loop through all parameters in the model\n",
    "        for name, param in param_dict.items():\n",
    "            if \"attention\" in name or \"attn\" in name:\n",
    "                # Add attention layer parameters to attention_params\n",
    "                attention_params.append(param)\n",
    "            else:\n",
    "                if param.dim() >= 2:\n",
    "                    decay_params.append(param)\n",
    "                else:\n",
    "                    nodecay_params.append(param)\n",
    "        \n",
    "        # It has been shown that weight decay induces low rank attention layers (https://arxiv.org/html/2410.23819v1) \n",
    "        optim_groups = [\n",
    "            {'params': attention_params, 'weight_decay': weight_decay*0.01},\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        num_attention_params = sum(p.numel() for p in attention_params)\n",
    "        print(f\"num attention parameter tensors: {len(attention_params)}, with {num_attention_params:,} parameters\")\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        optimizer = SOAP(optim_groups, lr=learning_rate, betas=betas, weight_decay=weight_decay, precondition_frequency=10)\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpbpoYQA5Iov"
   },
   "outputs": [],
   "source": [
    "asset_probs = [round(len(DFS_USED[s])/DATA_LEN, 2) for s in SYMBOLS]\n",
    "cat_ids = [9,10,11]\n",
    "cat_vars = [f'feature_{i:02}' for i in cat_ids]\n",
    "cont_vars = [f'feature_{i:02}' for i in range(79) if i not in cat_ids]\n",
    "responder_vars = [f'responder_{n}' for n in range(9)]\n",
    "print(responder_vars)\n",
    "\n",
    "mappings = {f'feature_{i:02}': {} for i in cat_ids}\n",
    "vocab_sizes = []\n",
    "for id in cat_ids:\n",
    "    f_name = f'feature_{id:02}'\n",
    "    unique_values = DATA.select(f_name).unique().collect().to_numpy().flatten()\n",
    "    mappings[f_name] = {val: idx for idx, val in enumerate(unique_values)}  # Map to range [0, len(unique_values)-1]\n",
    "    vocab_sizes.append(len(unique_values))  # Update vocab_size for this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea generating the batches is that we can separate the training set into days that have 848 steps and days that have 967 steps.\n",
    "# It's good to have the days that have the same length grouped together because the r2 weighted loss function will be closer to what it is evaluated.\n",
    "idxs_train_1 = []\n",
    "idxs_train_2 = []\n",
    "idxs_valid_1 = []\n",
    "idxs_valid_2 = []\n",
    "np.random.seed(42)\n",
    "count = 0\n",
    "for k, df in DFS_USED.items():\n",
    "    days_900 = df.filter(pl.col('time_id')>960).select(pl.col('date_id')).unique().to_numpy().flatten()\n",
    "    days_800 = df.filter(~pl.col('date_id').is_in(days_900)).select(pl.col('date_id')).unique().to_numpy().flatten()\n",
    "    days_900 = df.filter(pl.col('time_id')==0, pl.col('date_id').is_in(days_900)).select(pl.col('index')).to_numpy().flatten().tolist()\n",
    "    days_800 = df.filter(pl.col('time_id')==0, pl.col('date_id').is_in(days_800)).select(pl.col('index')).to_numpy().flatten().tolist()\n",
    "    days_800 = list(zip(days_800, [k] * len(days_800)))\n",
    "    idxs_train_1.extend(days_800[2:len(days_800)-5])\n",
    "    idxs_valid_1.extend(days_800[len(days_800)-5:len(days_800)])\n",
    "    \n",
    "    days_900 = list(zip(days_900, [k] * len(days_900)))\n",
    "    idxs_train_2.extend(days_900[2:len(days_900)-5])\n",
    "    idxs_valid_2.extend(days_900[len(days_900)-5:len(days_900)])\n",
    "\n",
    "print(len(idxs_train_1), len(idxs_valid_1), len(idxs_train_2), len(idxs_valid_2))\n",
    "np.random.shuffle(idxs_train_1)\n",
    "np.random.shuffle(idxs_valid_1)\n",
    "np.random.shuffle(idxs_train_2)\n",
    "np.random.shuffle(idxs_valid_2)\n",
    "\n",
    "# We will use the validation set for training on the final steps.\n",
    "idxs_train_1 = idxs_train_1 + idxs_valid_1\n",
    "idxs_train_2 = idxs_train_2 + idxs_valid_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COUNT_1 = 0\n",
    "TRAIN_COUNT_2 = 0\n",
    "EPOCH_COUNT = 0\n",
    "CHOSEN_TIME = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_COUNT_1 = 0\n",
    "VALID_COUNT_2 = 0\n",
    "CONT_COUNT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIfSo44ElJaE"
   },
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    global DATA\n",
    "    downsampled_block_size = 128 # We will mean pool block_size into 128 tokens. This allows for more information and not much is lost along the way\n",
    "\n",
    "    total_tokens = ((len(idxs_train_1) + len(idxs_train_2)) * downsampled_block_size)\n",
    "    print(\"The total amount of tokens is\", total_tokens)\n",
    "\n",
    "    eval_iters = downsampled_block_size\n",
    "    log_interval = 20\n",
    "\n",
    "    always_save_checkpoint = True\n",
    "\n",
    "    gradient_accumulation_steps = 32 # Below 32 scheduled sampling does not work\n",
    "    batch_size = 16 # Highest as memory allows\n",
    "    block_size = 896 # We will mean pool 896 tokens into 128 for the decoded sequence\n",
    "    \n",
    "    # Since both sequences are concatenated is important that they have the same block_size. If the attention strategy is changed this can be changed as well.\n",
    "    features_block_size = 32 # We will take the last 32 time steps of features\n",
    "    responder_block_size = 32 # We will take the responders from the previous day taking the current time_id as a pivot\n",
    "\n",
    "    tokens_per_iter = gradient_accumulation_steps * batch_size\n",
    "    print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "    n_epochs = 1\n",
    "    max_iters = int((total_tokens/tokens_per_iter)*n_epochs)\n",
    "    warmup_iters = int(max_iters*0.01)\n",
    "    lr_decay_iters = max_iters - warmup_iters\n",
    "\n",
    "    print(f\"We will train this dataset over {max_iters} steps\")\n",
    "    n_validations = 20 # The number of validations we will perform along the training\n",
    "    eval_interval = (max_iters//n_validations)-1\n",
    "\n",
    "    min_lr = 3e-5\n",
    "    # Since this objective function has a lot of local minima we need to tune the betas a bit.\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    weight_decay = 1e-1\n",
    "\n",
    "    # bfloat16 strongly preferred over float16. This allows us to double the batch size over float32.\n",
    "    dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16'\n",
    "    bias = False\n",
    "    decay_lr = True\n",
    "    eval_only = False\n",
    "    \n",
    "    # This can be set to 0.0 to disable gradient clipping, \n",
    "    #    but since sequences can be \"hard\" or \"easy\" it can make the loss jumpy, which can lead to big updates, so I strongly suggest it is used.\n",
    "    grad_clip = 1.0 \n",
    "\n",
    "    device = 'cuda' # Always cuda if available\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "    torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "    device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "    # note: float16 data type will automatically use a GradScaler\n",
    "    # This is needed for reproducibility in case we run this code with different GPUS\n",
    "\n",
    "    n_layer = 2\n",
    "    # 1 head per every 64 embd as rule of thumb\n",
    "    n_head = 2\n",
    "    n_embd = 128\n",
    "    bias = False\n",
    "    dropout = 0.0\n",
    "\n",
    "    # model init\n",
    "    model_args = dict(n_embd=n_embd, block_size=block_size, bias=bias, dropout=dropout, vocab_sizes=vocab_sizes, n_cat_features=len(cat_vars), n_cont_features=len(cont_vars), \n",
    "                      n_head=n_head, n_responder_features=len(responder_vars), n_layer=n_layer, vocab_size=64, n_iters=max_iters, \n",
    "                     downsampled_block_size=downsampled_block_size, features_block_size=features_block_size, responder_block_size=responder_block_size)\n",
    "\n",
    "    # This sets the matrix calculations precision to tensorfloat 32, which speeds up computation by a lot, with negligible cost for precision\n",
    "    # Check https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html for more info\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    conf = ModelConfig(**model_args)\n",
    "    model = Generator(conf)\n",
    "    print(model_args)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train_config = {\n",
    "        'vocab_size': 64, # Usually block_size // 2\n",
    "        'downsampled_block_size': downsampled_block_size,\n",
    "        'batch_size': batch_size,\n",
    "        'block_size': block_size,\n",
    "        'responder_block_size': responder_block_size,\n",
    "        'features_block_size': features_block_size,\n",
    "        'model_args': model_args,\n",
    "        'eval_interval': eval_interval,\n",
    "        'eval_iters': eval_iters,\n",
    "        'log_interval': log_interval,\n",
    "        'learning_rate': learning_rate,\n",
    "        'always_save_checkpoint': always_save_checkpoint,\n",
    "        'gradient_accumulation_steps': gradient_accumulation_steps,\n",
    "        'n_epochs': n_epochs,\n",
    "        'max_iters': max_iters,\n",
    "        'warmup_iters': warmup_iters,\n",
    "        'lr_decay_iters': lr_decay_iters,\n",
    "        'min_lr': min_lr,\n",
    "        'beta1': beta1,\n",
    "        'beta2': beta2,\n",
    "        'weight_decay': weight_decay,\n",
    "        'dtype': dtype,\n",
    "        'decay_lr': decay_lr,\n",
    "        'eval_only': eval_only,\n",
    "        'grad_clip': grad_clip,\n",
    "        'device': device,\n",
    "        'vocab_sizes': vocab_sizes\n",
    "    }\n",
    "\n",
    "    return model, train_config\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, ctx, train_config, it):\n",
    "    \"\"\"We will take the last 4 batches of training and the full validation set and see how they perform on a fully auto regressive sequence\"\"\"\n",
    "    global VALID_COUNT_1\n",
    "    global VALID_COUNT_2 \n",
    "    global TRAIN_COUNT_1\n",
    "    global TRAIN_COUNT_2\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    batch_train_count_1, batch_train_count_2 = TRAIN_COUNT_1, TRAIN_COUNT_2\n",
    "    batch_size = train_config['batch_size'] * 4\n",
    "    TRAIN_COUNT_1 -= batch_size\n",
    "    TRAIN_COUNT_2 -= batch_size\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(train_config['eval_iters'])\n",
    "        can_cont = False\n",
    "        prev_idxs = None\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        all_weights = []\n",
    "        # Create numpy arrays directly instead of accumulating lists\n",
    "        xpoints = np.zeros(train_config['eval_iters'])\n",
    "        ypoints = np.zeros(train_config['eval_iters'])\n",
    "\n",
    "        for k in range(train_config['eval_iters']):\n",
    "            if not can_cont:\n",
    "                X, Y, cat, cont, nan_mask, w, t_ids, rs, prev_idxs, can_cont = get_batch_new_ss(\n",
    "                    split, batch_size, train_config['block_size'], \n",
    "                    train_config['features_block_size'], \n",
    "                    train_config['responder_block_size'], \n",
    "                    train_config['device'], \n",
    "                    prev_idxs=None, \n",
    "                    ss=can_cont\n",
    "                )\n",
    "                synthetic_flag = torch.zeros(\n",
    "                    (batch_size, train_config['downsampled_block_size']), \n",
    "                    dtype=torch.int64, \n",
    "                    device=train_config['device']\n",
    "                )\n",
    "            else:\n",
    "                _, Y, cat, cont, nan_mask, w, t_ids, rs, prev_idxs, can_cont = update_batch(\n",
    "                    X, cat, cont, nan_mask, t_ids, rs\n",
    "                )\n",
    "                X = torch.cat([sampled[:, 1:], logits_detached], dim=1)\n",
    "\n",
    "            with ctx:\n",
    "                logits = model(\n",
    "                    X, cat, cont, nan_mask, rs, t_ids, \n",
    "                    targets=Y, weights=w, sampling_prob=1.0, \n",
    "                    synthetic_flag=synthetic_flag\n",
    "                )\n",
    "\n",
    "                # Store tensors directly without converting to CPU\n",
    "                all_logits.append(logits)\n",
    "                all_targets.append(Y)\n",
    "                all_weights.append(w)\n",
    "\n",
    "                # Store plotting data directly in numpy arrays\n",
    "                xpoints[k] = logits[0].item()  # Assumes single value needed\n",
    "                ypoints[k] = Y[0, 0].item()    # Assumes single value needed\n",
    "\n",
    "                sampled = X.clone()\n",
    "                logits_detached = logits.squeeze(-1).detach()\n",
    "                synthetic_flag = torch.cat([\n",
    "                    synthetic_flag[:, 1:], \n",
    "                    torch.ones((batch_size, 1), dtype=torch.int64, device=train_config['device'])\n",
    "                ], dim=1)\n",
    "\n",
    "        # Plot using the numpy arrays directly\n",
    "        plt.plot(xpoints, label='valid pred')\n",
    "        plt.plot(ypoints, label='valid real')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Concatenate all tensors while keeping them on GPU\n",
    "        combined_logits = torch.cat(all_logits, dim=0)\n",
    "        combined_targets = torch.cat(all_targets, dim=0)\n",
    "        combined_weights = torch.cat(all_weights, dim=0)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = zero_weighted_rsquared(\n",
    "            combined_logits.view(-1), \n",
    "            combined_targets.view(-1), \n",
    "            combined_weights.view(-1)\n",
    "        )\n",
    "        out[split] = loss.item()\n",
    "    model.train()\n",
    "    VALID_COUNT_1 = 0\n",
    "    VALID_COUNT_2 = 0\n",
    "    TRAIN_COUNT_1 = batch_train_count_1\n",
    "    TRAIN_COUNT_2 = batch_train_count_2\n",
    "    return out\n",
    "\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_sizes: list\n",
    "    block_size: int = 896\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = False # True: bias in Linears and LayerNorms. False: a bit better and faster\n",
    "    n_cat_features: int = 0\n",
    "    n_cont_features: int = 0\n",
    "    n_responder_features: int = 9\n",
    "    n_head: int = 2\n",
    "    n_layer: int = 2\n",
    "    vocab_size: int = 64\n",
    "    n_iters: int = 0\n",
    "    downsampled_block_size: int = 128\n",
    "    features_block_size: int = 32\n",
    "    responder_block_size: int = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We group the days depending on their length. \n",
    "# We randomly choose between using one or the other sets for each batch, and then we keep updating the batch until we finish the day.\n",
    "\n",
    "# We will use for context the last features_block_size. \n",
    "# For the responders we will take the current time, look it up on the day before and take the responder_block_size//2 before and after.\n",
    "# For example, if we are at time 200 and responder_block_size = 64 we will take all the responders from time 168 to 232.\n",
    "\n",
    "# For the decoded sequence we want to contain as much information from that day into the sequence, so we downsample it using mean pooling.\n",
    "# I noticed that using mean pooling on the sequence the r2 hovers around 0.85, so it does not loose too much information,\n",
    "#   and this means we can contain a sequence of length of around 850-950 in 128 tokens.\n",
    "\n",
    "BATCH_INFO = []\n",
    "def get_batch_new_ss(split, batch_size, block_size, features_block_size, responder_block_size, device='cuda', prev_idxs=None, ss=False, random=False):\n",
    "    # Pre-calculate constants\n",
    "    global CHOSEN_TIME\n",
    "    global BATCH_INFO\n",
    "    BATCH_INFO.clear()\n",
    "    downsample_period = block_size // 128\n",
    "    seq_block_size = 128\n",
    "    cutoff = 848 if CHOSEN_TIME == 0 else 967\n",
    "    \n",
    "    # Initialize tensors on CPU with proper memory layout. Change bfloat16 for float16 if not supported.\n",
    "    batch_tensors = {\n",
    "        'X': torch.zeros((batch_size, seq_block_size), dtype=torch.bfloat16),\n",
    "        'Y': torch.zeros((batch_size, 1), dtype=torch.bfloat16),\n",
    "        'cat': torch.zeros((batch_size, len(cat_vars), features_block_size), dtype=torch.int64),\n",
    "        'cont': torch.zeros((batch_size, len(cont_vars), features_block_size), dtype=torch.bfloat16),\n",
    "        'nan_cont': torch.zeros((batch_size, len(cont_vars), features_block_size), dtype=torch.int64),\n",
    "        't_ids': torch.zeros((batch_size, features_block_size), dtype=torch.int64),\n",
    "        'responders': torch.zeros((batch_size, len(responder_vars), responder_block_size), dtype=torch.bfloat16),\n",
    "        'weights': torch.zeros((batch_size, 1), dtype=torch.bfloat16)\n",
    "    }\n",
    "    \n",
    "    # Batch processing variables\n",
    "    idxs = []\n",
    "    can_cont = True\n",
    "    \n",
    "    # Choose time period if not in sequential mode\n",
    "    if not ss:\n",
    "        total_samples = len(idxs_train_1) + len(idxs_train_2)\n",
    "        p1 = len(idxs_train_1) / total_samples\n",
    "        CHOSEN_TIME = np.random.choice([0, 1], p=[p1, 1-p1])\n",
    "    \n",
    "    # Process each item in batch\n",
    "    for b in range(batch_size):\n",
    "        b_info = {}\n",
    "        # Get index and asset\n",
    "        if ss:\n",
    "            ix, asset = prev_idxs[b][0] + downsample_period, prev_idxs[b][1]\n",
    "        else:\n",
    "            ix, asset = _get_index_and_asset(split, random, cutoff)\n",
    "        # print(ix, asset)\n",
    "        b_info['idx'] = ix\n",
    "        b_info['asset'] = asset\n",
    "        b_info['offset'] = downsample_period\n",
    "        idxs.append((ix, asset))\n",
    "        \n",
    "        # Get data frames and process sequence\n",
    "        df = DFS_USED[asset]\n",
    "        dates = DATES_USED[asset]\n",
    "        sequence_data = df.slice(ix-block_size, block_size+downsample_period)\n",
    "        \n",
    "        # Process time and date information\n",
    "        date = sequence_data['date_id'].to_numpy()[-1]\n",
    "        time = sequence_data['time_id'].to_numpy()[-downsample_period]\n",
    "        b_info['time'] = time\n",
    "        \n",
    "        if time + downsample_period*2 > cutoff:\n",
    "            can_cont = False\n",
    "            \n",
    "        # Process responder data\n",
    "        prev_date, second_prev_date = _get_previous_date(dates, date)\n",
    "        responders_data = _process_responders(df, ix, prev_date, second_prev_date, time, responder_block_size, b_info)\n",
    "        batch_tensors['responders'][b] = torch.tensor(\n",
    "            responders_data[responder_vars].to_numpy().T[-responder_block_size:]\n",
    "        )\n",
    "        \n",
    "        # Process main sequence data\n",
    "        sequence = sequence_data['responder_6'].to_numpy().flatten()\n",
    "        downsampled = downsample_array(sequence, downsample_period)\n",
    "        batch_tensors['X'][b] = torch.tensor(downsampled[-(seq_block_size+1):-1].astype(np.float32))\n",
    "        batch_tensors['Y'][b] = torch.tensor(np.clip(downsampled[-1], -5, 5).astype(np.float32))\n",
    "        \n",
    "        # Process time IDs\n",
    "        time_slice = slice(-(features_block_size+downsample_period-1), -(downsample_period-1))\n",
    "        batch_tensors['t_ids'][b] = torch.tensor(sequence_data['time_id'][time_slice].to_numpy())\n",
    "        \n",
    "        # Process categorical variables\n",
    "        for f_id, f in enumerate(cat_vars):\n",
    "            mapped_cat = np.vectorize(mappings[f].get)(sequence_data[f].to_numpy()[time_slice])\n",
    "            batch_tensors['cat'][b, f_id] = torch.tensor(mapped_cat)\n",
    "        \n",
    "        # Process continuous variables\n",
    "        cont_data = sequence_data[time_slice]\n",
    "        for f_id, f in enumerate(cont_vars):\n",
    "            seq = cont_data[f].to_numpy()\n",
    "            batch_tensors['cont'][b, f_id] = torch.tensor(np.nan_to_num(seq).astype(np.float32))\n",
    "            batch_tensors['nan_cont'][b, f_id] = torch.tensor((~np.isnan(seq)).astype(np.int64))\n",
    "        \n",
    "        # Process weights\n",
    "        batch_tensors['weights'][b] = torch.tensor(\n",
    "            np.mean(sequence_data['weight'][-downsample_period:].to_numpy().flatten())\n",
    "        )\n",
    "        BATCH_INFO.append(b_info)\n",
    "    \n",
    "    # Move tensors to device efficiently\n",
    "    if device == 'cuda':\n",
    "        device_tensors = {\n",
    "            k: v.pin_memory().to(device, non_blocking=True) \n",
    "            for k, v in batch_tensors.items()\n",
    "        }\n",
    "    else:\n",
    "        device_tensors = {\n",
    "            k: v.to(device) for k, v in batch_tensors.items()\n",
    "        }\n",
    "    \n",
    "    return (\n",
    "        device_tensors['X'], device_tensors['Y'], device_tensors['cat'],\n",
    "        device_tensors['cont'], device_tensors['nan_cont'], device_tensors['weights'],\n",
    "        device_tensors['t_ids'], device_tensors['responders'], idxs, can_cont\n",
    "    )\n",
    "\n",
    "def _get_index_and_asset(split, random, cutoff):\n",
    "    \"\"\"Helper function to get index and asset based on split type\"\"\"\n",
    "    global TRAIN_COUNT_1, TRAIN_COUNT_2, VALID_COUNT_1, VALID_COUNT_2, EPOCH_COUNT\n",
    "    \n",
    "    # Reset counters if needed\n",
    "    if TRAIN_COUNT_1 >= len(idxs_train_1):\n",
    "        TRAIN_COUNT_1 = 0\n",
    "        EPOCH_COUNT += 1\n",
    "    if TRAIN_COUNT_2 >= len(idxs_train_2):\n",
    "        TRAIN_COUNT_2 = 0\n",
    "        EPOCH_COUNT += 1\n",
    "    if VALID_COUNT_1 >= len(idxs_valid_1):\n",
    "        VALID_COUNT_1 = 0\n",
    "    if VALID_COUNT_2 >= len(idxs_valid_2):\n",
    "        VALID_COUNT_2 = 0\n",
    "    \n",
    "    # Get appropriate index and asset\n",
    "    if split == 'train':\n",
    "        if CHOSEN_TIME == 0:\n",
    "            if random:\n",
    "                ix, asset = idxs_train_1[np.random.choice(len(idxs_train_1))]\n",
    "                ix = ix + np.random.randint(cutoff-1)\n",
    "            else:\n",
    "                ix, asset = idxs_train_1[TRAIN_COUNT_1]\n",
    "                TRAIN_COUNT_1 += 1\n",
    "        else:\n",
    "            if random:\n",
    "                ix, asset = idxs_train_2[np.random.choice(len(idxs_train_2))]\n",
    "                ix = ix + np.random.randint(cutoff-1)\n",
    "            else:\n",
    "                ix, asset = idxs_train_2[TRAIN_COUNT_2]\n",
    "                TRAIN_COUNT_2 += 1\n",
    "    else:\n",
    "        if CHOSEN_TIME == 0:\n",
    "            ix, asset = idxs_valid_1[VALID_COUNT_1]\n",
    "            VALID_COUNT_1 += 1\n",
    "        else:\n",
    "            ix, asset = idxs_valid_2[VALID_COUNT_2]\n",
    "            VALID_COUNT_2 += 1\n",
    "            \n",
    "    return ix, asset\n",
    "\n",
    "def _get_previous_date(dates, date):\n",
    "    \"\"\"Helper function to get the previous date\"\"\"\n",
    "    prev_date = None\n",
    "    second_prev_date = None\n",
    "    for d in dates:\n",
    "        if d == date:\n",
    "            break\n",
    "        second_prev_date = prev_date\n",
    "        prev_date = d\n",
    "    return prev_date, second_prev_date\n",
    "\n",
    "def _process_responders(df, ix, prev_date, second_prev_date, t, responder_block_size, b_info):\n",
    "    \"\"\"Helper function to process responder data\"\"\"\n",
    "    prev_day = df.filter(pl.col(\"date_id\") <= prev_date, pl.col(\"date_id\") >= second_prev_date).drop(['index']).with_row_index()\n",
    "    b_info['resp'] = prev_day\n",
    "    to_t = min(t + responder_block_size//2, len(prev_day))\n",
    "    idx_to_cut = prev_day.filter(pl.col(\"time_id\") < to_t)['index'][-1]\n",
    "    b_info['resp_cut'] = idx_to_cut\n",
    "    b_info['resp_max_index'] = prev_day.shape[0] - 1\n",
    "    to_return = prev_day.slice(idx_to_cut - responder_block_size + 1, responder_block_size)\n",
    "    return to_return\n",
    "\n",
    "def update_batch(x, cat, cont, nan_cont, t_ids, responders):\n",
    "    \"\"\"We use the batch information generated to increase the idx by 1 and continue the sequence in a compute efficient way\"\"\"\n",
    "    global BATCH_INFO\n",
    "    batch_size, block_size = x.shape[0], 896\n",
    "    features_block_size = cont.shape[2]\n",
    "    responder_block_size = responders.shape[2]\n",
    "    device = x.device\n",
    "    downsample_period = block_size // 128\n",
    "    seq_block_size = 128\n",
    "    cutoff = 848 if CHOSEN_TIME == 0 else 967\n",
    "    \n",
    "    new_x = torch.zeros((batch_size, 1), dtype=torch.bfloat16, device=device)\n",
    "    new_y = torch.zeros((batch_size, 1), dtype=torch.bfloat16, device=device)\n",
    "    new_weights = torch.zeros((batch_size, 1), dtype=torch.bfloat16, device=device)\n",
    "    # new_t_ids = torch.zeros((batch_size, downsample_period), dtype=torch.int64)\n",
    "    new_cat = torch.zeros((batch_size, len(cat_vars), downsample_period), dtype=torch.int64, device=device)\n",
    "    new_cont = torch.zeros((batch_size, len(cont_vars), downsample_period), dtype=torch.bfloat16, device=device)\n",
    "    new_nan_cont = torch.zeros((batch_size, len(cont_vars), downsample_period), dtype=torch.int64, device=device)\n",
    "    new_rs = torch.zeros((batch_size, len(responder_vars), downsample_period), dtype=torch.bfloat16, device=device)\n",
    "    \n",
    "    # Batch processing variables\n",
    "    idxs = []\n",
    "    can_cont = True\n",
    "    \n",
    "    # Process each item in batch\n",
    "    for b in range(batch_size):\n",
    "        # Get index and asset\n",
    "        ix, asset = BATCH_INFO[b]['idx'] + BATCH_INFO[b]['offset'], BATCH_INFO[b]['asset']\n",
    "        # print(ix, asset)\n",
    "        idxs.append((ix, asset))\n",
    "        \n",
    "        # Get data frames and process sequence\n",
    "        df = DFS_USED[asset]\n",
    "        dates = DATES_USED[asset]\n",
    "        sequence_data = df.slice(ix-downsample_period, downsample_period*2)\n",
    "        t = BATCH_INFO[b]['time'] + BATCH_INFO[b]['offset']\n",
    "        if t + downsample_period*2 > cutoff:\n",
    "            can_cont = False\n",
    "        \n",
    "        new_x[b] = torch.tensor(np.mean(sequence_data['responder_6'].to_numpy()[:downsample_period]).astype(np.float32))\n",
    "        new_y[b] = torch.tensor([np.mean(sequence_data['responder_6'].to_numpy()[-downsample_period:])])\n",
    "        new_weights[b] = torch.tensor([np.mean(sequence_data['weight'].to_numpy()[-downsample_period:])])\n",
    "        for f_id, f in enumerate(cat_vars):\n",
    "            mapped_cat = np.vectorize(mappings[f].get)(sequence_data[f].to_numpy()[1:downsample_period+1])\n",
    "            new_cat[b, f_id] = torch.tensor(mapped_cat)\n",
    "\n",
    "        cont_data = sequence_data[1:downsample_period+1]\n",
    "        for f_id, f in enumerate(cont_vars):\n",
    "            seq = cont_data[f].to_numpy()\n",
    "            new_cont[b, f_id] = torch.tensor(np.nan_to_num(seq).astype(np.float32))\n",
    "            new_nan_cont[b, f_id] = torch.tensor((~np.isnan(seq)).astype(np.int64))\n",
    "        \n",
    "        prev_day = BATCH_INFO[b]['resp']\n",
    "        idx_to_cut = BATCH_INFO[b]['resp_cut'] + BATCH_INFO[b]['offset']\n",
    "        max_index = BATCH_INFO[b]['resp_max_index']\n",
    "        \n",
    "        if idx_to_cut > max_index:\n",
    "            period = max(0, downsample_period - (idx_to_cut - max_index))\n",
    "        else:\n",
    "            period = downsample_period\n",
    "        idx_to_cut = min(idx_to_cut, max_index)\n",
    "        responders_data = prev_day.slice(idx_to_cut - responder_block_size + 1, responder_block_size)\n",
    "        #print(responders_data[responder_vars].to_numpy().T[:, -downsample_period:])\n",
    "        new_rs[b] = torch.tensor(\n",
    "            responders_data[responder_vars].to_numpy().T[:, -downsample_period:], dtype=torch.bfloat16, device=device\n",
    "        )\n",
    "        BATCH_INFO[b]['offset'] += downsample_period\n",
    "    x = torch.cat([x[:, 1:], new_x], dim=1)\n",
    "    cat = torch.cat([cat[:, :, downsample_period:], new_cat], dim=2)\n",
    "    t_ids = torch.cat([\n",
    "        t_ids[:, downsample_period:], \n",
    "        torch.tensor(\n",
    "            sequence_data['time_id'][1:downsample_period+1].to_numpy(), dtype=torch.int64, device=device\n",
    "        ).repeat(batch_size, 1)\n",
    "    ], dim=1)\n",
    "    cont = torch.cat([cont[:, :, downsample_period:], new_cont], dim=2)\n",
    "    nan_cont = torch.cat([nan_cont[:, :, downsample_period:], new_nan_cont], dim=2)\n",
    "    if period > 0:\n",
    "        responders = torch.cat([responders[:, :, period:], new_rs[:, :, -period:]], dim=2)\n",
    "    return (\n",
    "        x, new_y, cat,\n",
    "        cont, nan_cont, new_weights,\n",
    "        t_ids, responders, idxs, can_cont\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJv3_qHo_XVp",
    "outputId": "281ca25c-a571-4b22-8c64-c21247f8e532",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model, train_config = prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGfqTpf2eLz6"
   },
   "outputs": [],
   "source": [
    "def get_ss_prob(it, train_config):\n",
    "    # no-op\n",
    "    return train_config['downsampled_block_size']\n",
    "\n",
    "def get_lr(it, train_config):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < train_config['warmup_iters']:\n",
    "        return train_config['learning_rate'] * it / train_config['warmup_iters']\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > train_config['lr_decay_iters']:\n",
    "        return train_config['min_lr']\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - train_config['warmup_iters']) / (train_config['lr_decay_iters'] - train_config['warmup_iters'])\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return train_config['min_lr'] + coeff * (train_config['learning_rate'] - train_config['min_lr'])\n",
    "\n",
    "def main(init_mode='scratch', save_at_end=True, savename='ckpt', out_dir='models', load_only=False, checkpoint_name='ckpt.pt'):\n",
    "    global EPOCH_COUNT\n",
    "    global TRAIN_COUNT_1\n",
    "    global VALID_COUNT_1\n",
    "    global TRAIN_COUNT_2\n",
    "    global VALID_COUNT_2\n",
    "    global idxs_train_1\n",
    "    global idxs_train_2\n",
    "    \n",
    "    if save_at_end:\n",
    "        assert isinstance(savename, str), \"The name of the model hast to be a str\"\n",
    "    \n",
    "    if init_mode == 'resume':\n",
    "        print(f\"Resuming training from {out_dir}\")\n",
    "        # resume training from a checkpoint.\n",
    "        ckpt_path = os.path.join(out_dir, checkpoint_name)\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "        #_, train_config = prepare_model()\n",
    "        train_config = checkpoint['train_config']\n",
    "        \n",
    "        # In case the checkpoint was not correctly saved\n",
    "        if 'vocab_sizes' not in train_config:\n",
    "            train_config['vocab_sizes'] = vocab_sizes\n",
    "        if 'model_args' not in train_config:\n",
    "            model_args = dict(n_embd=64, block_size=896, bias=False, dropout=0.0, vocab_sizes=vocab_sizes, n_cat_features=len(cat_vars), n_cont_features=len(cont_vars))\n",
    "            train_config['model_args'] = model_args\n",
    "            \n",
    "        torch.set_float32_matmul_precision('high')\n",
    "                \n",
    "        model_args = dict(n_embd=train_config['model_args']['n_embd'], block_size=train_config['model_args']['block_size'], bias=train_config['model_args']['bias'],\n",
    "                          features_block_size=train_config['features_block_size'], responder_block_size=train_config['responder_block_size'],\n",
    "                          dropout=train_config['model_args']['dropout'], vocab_sizes=train_config['vocab_sizes'], n_cat_features=len(cat_vars), n_cont_features=len(cont_vars),\n",
    "                          n_head=train_config['model_args']['n_head'], n_layer=train_config['model_args']['n_layer'], n_iters=train_config['max_iters'], \n",
    "                          downsampled_block_size=train_config['model_args']['downsampled_block_size'])\n",
    "        \n",
    "        conf = ModelConfig(**model_args)\n",
    "        model = Generator(conf)\n",
    "        state_dict = checkpoint['model']\n",
    "\n",
    "        # Not sure why the model is saved with this prefix...\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k,v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        state_dict = {k: v for k, v in state_dict.items() if k in model.state_dict()}\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "        if load_only:\n",
    "            model.to(train_config['device'])\n",
    "            return model\n",
    "        \n",
    "        iter_num = checkpoint['iter_num'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        EPOCH_COUNT = checkpoint['epoch_count']\n",
    "        TRAIN_COUNT_1 = checkpoint['train_count_1']\n",
    "        VALID_COUNT_1 = checkpoint['valid_count_1']\n",
    "        TRAIN_COUNT_2 = checkpoint['train_count_2']\n",
    "        VALID_COUNT_2 = checkpoint['valid_count_2']\n",
    "        print(f\"Epoch: {EPOCH_COUNT}, Train sample: {TRAIN_COUNT_1}, Valid sample: {VALID_COUNT_1}. Starting at {iter_num}\")\n",
    "    else:\n",
    "        model, train_config = prepare_model()\n",
    "        iter_num = 0\n",
    "        best_val_loss = 1e9\n",
    "        \n",
    "    print(\"Testing batch...\")\n",
    "    X, Y, cat, cont, nan_mask, w, t_ids, rs, prev_idxs, can_cont = get_batch_new_ss('train', train_config['batch_size'], train_config['block_size'], train_config['features_block_size'], train_config['responder_block_size'], train_config['device'])\n",
    "    print(X.shape, Y.shape, cat.shape, cont.shape, nan_mask.shape, t_ids.shape, rs.shape, can_cont)\n",
    "\n",
    "    assert Y.size() == (train_config['batch_size'], 1)\n",
    "    assert X.size() == (train_config['batch_size'],train_config['downsampled_block_size'])\n",
    "    assert cat.size() == (train_config['batch_size'], len(cat_vars),train_config['features_block_size'])\n",
    "    assert cont.size() == (train_config['batch_size'], len(cont_vars), train_config['features_block_size'])\n",
    "    assert nan_mask.size() == (train_config['batch_size'], len(cont_vars), train_config['features_block_size'])\n",
    "    assert w.size() == (train_config['batch_size'],1)\n",
    "    assert t_ids.size() == (train_config['batch_size'],train_config['features_block_size'])\n",
    "    assert rs.size() == (train_config['batch_size'],len(responder_vars), train_config['responder_block_size'])\n",
    "\n",
    "    print(\"Test succesful!\")\n",
    "\n",
    "    # We pass the model to the device\n",
    "    model.to(train_config['device'])\n",
    "    # initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(train_config['dtype'] == 'float16'))\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = model.configure_optimizers(train_config['weight_decay'], train_config['learning_rate'], (train_config['beta1'], train_config['beta2']), train_config['device'])\n",
    "    if init_mode == 'resume':\n",
    "        print(\"Loading optimizer...\")\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    # NOTE: Compile the model if we are running in a gpu!\n",
    "    # If using RoPE the compiling raises some warnings, just ignore them...\n",
    "    if True: \n",
    "        print(\"compiling the model... (takes a ~minute)\")\n",
    "        model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "    t0 = time.time()\n",
    "    raw_model = model\n",
    "    outs = []\n",
    "    ss_counter = 0\n",
    "    # The synthetic flag is a tensor that represents if a token in the decoded sequence is model generated or is a golden token.\n",
    "    synthetic_flag = torch.zeros((train_config['batch_size'], train_config['downsampled_block_size']), dtype=torch.int64, device=train_config['device'])\n",
    "    update_sample = False \n",
    "    xpoints = []\n",
    "    ypoints = []\n",
    "    wpoints = []\n",
    "    scheduled_sampling_enabled = True\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[train_config['dtype']]\n",
    "    ctx = nullcontext() if train_config['device'] == 'cpu' else torch.amp.autocast(device_type=train_config['device'], dtype=ptdtype)\n",
    "    # In case we want to cut schedule sampling at some point to avoid fully model-generated sequences.\n",
    "    # Since the scheduler is exponential I found that cutting at 0.5 is a good compromise.\n",
    "    cutoff_pct = 1 # 0.5\n",
    "    cutoff_iter = int(train_config['max_iters']*cutoff_pct)\n",
    "    print(f\"Cutting scheduled sampling at {cutoff_iter}\")\n",
    "    print(\"#### Starting Training ####\")\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num, train_config) if train_config['decay_lr'] else train_config['learning_rate']\n",
    "        sampled_tokens = int(get_ss_prob(iter_num, train_config))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num % train_config['eval_interval'] == 0 and iter_num > 0:\n",
    "            losses =  estimate_loss(model, train_config, iter_num)\n",
    "            val_line = f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "            outs.append(val_line)\n",
    "            print(val_line)\n",
    "            can_cont = False\n",
    "            if losses['val'] < best_val_loss or train_config['always_save_checkpoint']:\n",
    "                best_val_loss = losses['val']\n",
    "                with open('out.txt', 'w') as file:\n",
    "                    file.write('\\n'.join(outs) + '\\n')\n",
    "                # Uncomment this for checkpoint saving\n",
    "                if iter_num > 0:\n",
    "                   checkpoint = {\n",
    "                       'model': raw_model.state_dict(),\n",
    "                       'optimizer': optimizer.state_dict(),\n",
    "                       'iter_num': iter_num,\n",
    "                       'best_val_loss': best_val_loss,\n",
    "                       'epoch_count': EPOCH_COUNT,\n",
    "                       'train_count_1': TRAIN_COUNT_1,\n",
    "                       'valid_count_1': VALID_COUNT_1,\n",
    "                       'train_count_2': TRAIN_COUNT_2,\n",
    "                       'valid_count_2': VALID_COUNT_2,\n",
    "                       'train_config': train_config,\n",
    "                       'mappings': mappings,\n",
    "                       'cat_ids': cat_ids\n",
    "                   }\n",
    "                   print(f\"saving checkpoint to {out_dir}\")\n",
    "                   torch.save(checkpoint, os.path.join(out_dir, f'ckpt_{iter_num}_2.pt'))\n",
    "        if iter_num == 0 and train_config['eval_only']:\n",
    "            break\n",
    "\n",
    "        # This does {gradient_accumulation_steps} of forward steps, and performs a backwards pass after that.\n",
    "        # Depending on the iter_num and decoded_step the model will have a higher probability to use its own predictions as context. \n",
    "        # This is called scheduled sampling, and is used in order to reduce exposure bias in auto regressive inference scenarios. \n",
    "        # (https://arxiv.org/abs/1506.03099 for the paper introducting scheduled sampling, https://aclanthology.org/2021.emnlp-main.264.pdf for the introduction of decoder-step-based probabilities)\n",
    "        # Since the loss function is a weighted r2, it is imperative that the gradient_accumulation_steps are high (probably at least 32).\n",
    "        # This is because in smaller values the loss can be quite jumpy (for example an \"easy\" sequence can have a loss of 0.5, while a \"difficult\" one can go up to 1), which makes the learning difficult.\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "        all_weights = []\n",
    "        \n",
    "        for micro_step in range(train_config['gradient_accumulation_steps']):\n",
    "            # The max decoder step can count as an hyperparameter to tune.\n",
    "            # In a case where the information of the sequence is lost on the first tokens is sensible to set it low so the model sampling probability is high.\n",
    "            sampling_prob = composite_schedule_sampling(min(cutoff_iter, iter_num), max(ss_counter, 10), train_config['max_iters'], 7)\n",
    "\n",
    "            with ctx:\n",
    "                logits = model(X, cat, cont, nan_mask, rs, t_ids,\n",
    "                               it=iter_num, synthetic_flag=synthetic_flag)\n",
    "                if not scheduled_sampling_enabled:\n",
    "                    # If we don't use scheduled sampling we can calculate the loss and do the backwards pass in the loop.\n",
    "                    loss = zero_weighted_rsquared(logits.view(-1), Y.view(-1), w.view(-1))\n",
    "                    loss = loss / train_config['gradient_accumulation_steps'] # scale the loss to account for gradient accumulation\n",
    "                else:\n",
    "                    # Store predictions and targets\n",
    "                    all_logits.append(logits)\n",
    "                    all_targets.append(Y)\n",
    "                    all_weights.append(w)\n",
    "                \n",
    "            if not can_cont:\n",
    "                ss_counter = 0\n",
    "                # Uncomment this to see the generated predictions.\n",
    "                #plt.plot(xpoints, label='pred')\n",
    "                #plt.plot(ypoints, label='real')\n",
    "                #plt.legend()\n",
    "                #plt.show()\n",
    "                #print(\"Loss: \", zero_weighted_rsquared_np(np.array(xpoints).flatten(), np.array(ypoints).flatten(), np.array(wpoints).flatten()))\n",
    "                #xpoints = []\n",
    "                #ypoints = []\n",
    "                #wpoints = []\n",
    "                synthetic_flag = torch.zeros((train_config['batch_size'], \n",
    "                                            train_config['downsampled_block_size']), \n",
    "                                           dtype=torch.int64, \n",
    "                                           device=train_config['device'])\n",
    "                update_sample = False\n",
    "                \n",
    "            if sampled_tokens > 0 and can_cont:\n",
    "                sampled = X.clone()\n",
    "                update_sample = True\n",
    "                logits_detached = logits.squeeze(-1).detach()\n",
    "                \n",
    "            if sampled_tokens == 0:\n",
    "                can_cont = False\n",
    "                \n",
    "            # Uncomment this to see the generated predictions\n",
    "            #if update_sample:\n",
    "            #    xpoints.append(logits_detached[0].cpu().numpy())\n",
    "            #    ypoints.append(Y.to(dtype=torch.float32).detach().cpu().numpy()[0][0])\n",
    "            #    wpoints.append(w.to(dtype=torch.float32).detach().cpu().numpy()[0][0])\n",
    "\n",
    "            if can_cont:\n",
    "                X, Y, cat, cont, nan_mask, w, t_ids, rs, prev_idxs, can_cont = update_batch(\n",
    "                    X.detach(),\n",
    "                    cat.detach(),\n",
    "                    cont.detach(),\n",
    "                    nan_mask.detach(),\n",
    "                    t_ids.detach(),\n",
    "                    rs.detach()\n",
    "                )\n",
    "            else:\n",
    "                X, Y, cat, cont, nan_mask, w, t_ids, rs, prev_idxs, can_cont = get_batch_new_ss(\n",
    "                    'train', \n",
    "                    train_config['batch_size'], \n",
    "                    train_config['block_size'], \n",
    "                    train_config['features_block_size'], \n",
    "                    train_config['responder_block_size'], \n",
    "                    train_config['device'], \n",
    "                    prev_idxs, \n",
    "                    ss=can_cont,\n",
    "                    random=False\n",
    "                )\n",
    "            \n",
    "            if update_sample:\n",
    "                ss_counter += 1\n",
    "                # Random values for each sequence\n",
    "                use_prediction = torch.rand((train_config['batch_size'], 1), device=train_config['device'])\n",
    "                # Whether we want to use the golden tokens or the model prediction\n",
    "                to_cat = torch.where(use_prediction < sampling_prob, logits_detached, X[:, [-1]])\n",
    "                X = torch.cat([sampled[:, 1:], to_cat], dim=1)\n",
    "                \n",
    "                synthetic_flag = torch.cat([\n",
    "                    synthetic_flag[:, 1:],\n",
    "                    torch.where(\n",
    "                        use_prediction < sampling_prob,\n",
    "                        1,  # model generated context\n",
    "                        0   # golden token\n",
    "                    )\n",
    "                ], dim=1)\n",
    "\n",
    "            if not scheduled_sampling_enabled:    \n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "        # Combine all stored tensors\n",
    "        if scheduled_sampling_enabled:\n",
    "            combined_logits = torch.cat(all_logits, dim=0)\n",
    "            combined_targets = torch.cat(all_targets, dim=0)\n",
    "            combined_weights = torch.cat(all_weights, dim=0)\n",
    "\n",
    "            loss = zero_weighted_rsquared(combined_logits.view(-1), combined_targets.view(-1), combined_weights.view(-1))\n",
    "        \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "        if train_config['grad_clip'] != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            norm = torch.nn.utils.clip_grad_norm_(model.parameters(), train_config['grad_clip'])\n",
    "        else:\n",
    "            norm = 0\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # timing and logging\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % train_config['log_interval'] == 0:\n",
    "            mul = train_config['gradient_accumulation_steps'] if not scheduled_sampling_enabled else 1\n",
    "            lossf = loss.item() * mul\n",
    "            max_iters = train_config[\"max_iters\"]\n",
    "            log_line = f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, norm: {norm:.4f}, sampling prob: {sampling_prob}\"\n",
    "            outs.append(log_line)\n",
    "            print(log_line)\n",
    "        iter_num += 1\n",
    "\n",
    "        # termination conditions\n",
    "        if iter_num > train_config['max_iters']:\n",
    "            break\n",
    "            \n",
    "    if save_at_end:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        checkpoint = {\n",
    "            'model': raw_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'iter_num': iter_num,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'epoch_count': EPOCH_COUNT,\n",
    "            'train_count_1': TRAIN_COUNT_1,\n",
    "            'valid_count_1': VALID_COUNT_1,\n",
    "            'train_count_2': TRAIN_COUNT_2,\n",
    "            'valid_count_2': VALID_COUNT_2,\n",
    "            'train_config': train_config,\n",
    "            'mappings': mappings,\n",
    "            'cat_ids': cat_ids\n",
    "        }\n",
    "        print(f\"saving checkpoint to {out_dir}\")\n",
    "        torch.save(checkpoint, os.path.join(out_dir, f'{savename}.pt'))\n",
    "        \n",
    "    with open('out.txt', 'w') as file:\n",
    "        file.write('\\n'.join(outs) + '\\n')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "ysxPPBudWdQu",
    "outputId": "2f2e4740-c287-455a-c074-b13efc41772d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = main(init_mode='scratch', save_at_end=True, savename='ckpt_final', load_only=False, checkpoint_name='full_1/ckpt_10574_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iT3N9lncUZXf",
    "outputId": "4faddf80-01ac-400a-f0bd-91fb6c58ae4b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
